{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# Configurations\n",
    "THRESHOLD = 0.45\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "EYE_AR_THRESH = 0.25  # Eye Aspect Ratio threshold\n",
    "EYE_AR_CONSEC_FRAMES = 3  # Frames to confirm blink\n",
    "REQUIRED_BLINKS = 2  # Minimum blinks to verify liveness\n",
    "HEAD_MOVEMENT_THRESH = 15  # Pixel movement threshold for head motion\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            known_face_encodings, known_face_names = pickle.load(f)\n",
    "        return known_face_encodings, known_face_names\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found. Ensure it exists.\")\n",
    "        return [], []\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    \"\"\"Compute Eye Aspect Ratio (EAR) for blink detection\"\"\"\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def is_real_face(landmarks, prev_face_pos):\n",
    "    \"\"\"Check liveness using blink + head movement + texture\"\"\"\n",
    "    # 1. Blink Detection\n",
    "    left_eye = landmarks[\"left_eye\"]\n",
    "    right_eye = landmarks[\"right_eye\"]\n",
    "    left_ear = eye_aspect_ratio(left_eye)\n",
    "    right_ear = eye_aspect_ratio(right_eye)\n",
    "    ear = (left_ear + right_ear) / 2.0\n",
    "    is_blinking = ear < EYE_AR_THRESH\n",
    "\n",
    "    # 2. Head Movement (Compare nose position)\n",
    "    nose = landmarks[\"nose_tip\"][0]\n",
    "    if prev_face_pos:\n",
    "        movement = dist.euclidean(nose, prev_face_pos)\n",
    "        has_moved = movement > HEAD_MOVEMENT_THRESH\n",
    "    else:\n",
    "        has_moved = False\n",
    "\n",
    "    # 3. Texture Analysis (Brightness variance)\n",
    "    # (Note: Requires face_roi, but we don't have it here. Skip or add later.)\n",
    "    is_live = is_blinking or has_moved\n",
    "    return is_live, nose\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names, prev_face_pos, liveness_counter):\n",
    "    \"\"\"Recognize faces with liveness checks\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame, face_locations)\n",
    "\n",
    "    for (top, right, bottom, left), face_encoding, landmarks in zip(\n",
    "        face_locations, face_encodings, face_landmarks_list\n",
    "    ):\n",
    "        # Liveness Check\n",
    "        is_live, nose_pos = is_real_face(landmarks, prev_face_pos)\n",
    "        if is_live:\n",
    "            liveness_counter += 1\n",
    "            prev_face_pos = nose_pos\n",
    "\n",
    "        # Only recognize after liveness confirmation\n",
    "        if liveness_counter >= REQUIRED_BLINKS:\n",
    "            face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "            best_match_idx = np.argmin(face_distances)\n",
    "            best_distance = face_distances[best_match_idx]\n",
    "            name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "            color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "        else:\n",
    "            name = \"Move/Blink to verify\"\n",
    "            color = (255, 255, 0)  # Yellow (pending)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    return frame, prev_face_pos, liveness_counter\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    if not known_encodings:\n",
    "        return  # Exit if no known faces loaded\n",
    "\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    prev_face_pos = None\n",
    "    liveness_counter = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame, prev_face_pos, liveness_counter = recognize_faces(\n",
    "            frame, known_encodings, known_names, prev_face_pos, liveness_counter\n",
    "        )\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 0.45  # Similarity threshold\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "MIN_FACE_SIZE = 100  # Minimum pixel size for a face to be considered\n",
    "BLUR_THRESHOLD = 100  # Threshold for blur detection (lower is more blurry)\n",
    "SCREEN_REFLECTION_THRESHOLD = 200  # Brightness threshold for screen detection\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file with error handling\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            known_face_encodings, known_face_names = pickle.load(f)\n",
    "        return known_face_encodings, known_face_names\n",
    "    except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "        print(f\"Error loading face database: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def is_screen_face(frame, face_location):\n",
    "    \"\"\"Check if the face is likely on a screen (photo/video)\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    face_region = frame[top:bottom, left:right]\n",
    "    gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray_face)\n",
    "    return avg_brightness > SCREEN_REFLECTION_THRESHOLD\n",
    "\n",
    "def is_too_blurry(frame):\n",
    "    \"\"\"Check if the frame is too blurry for reliable detection\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() < BLUR_THRESHOLD\n",
    "\n",
    "def is_face_too_small(face_location):\n",
    "    \"\"\"Check if the detected face is too small to be real\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    return (right - left) < MIN_FACE_SIZE or (bottom - top) < MIN_FACE_SIZE\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names):\n",
    "    \"\"\"Enhanced face recognition with filtering\"\"\"\n",
    "    if is_too_blurry(frame):\n",
    "        cv2.putText(frame, \"Low quality - ignoring\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        return frame\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    \n",
    "    for face_location in face_locations:\n",
    "        if is_face_too_small(face_location) or is_screen_face(frame, face_location):\n",
    "            continue\n",
    "            \n",
    "        face_encoding = face_recognition.face_encodings(rgb_frame, [face_location])[0]\n",
    "        face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "        best_match_idx = np.argmin(face_distances)\n",
    "        best_distance = face_distances[best_match_idx]\n",
    "        \n",
    "        name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "        (top, right, bottom, left) = face_location\n",
    "        \n",
    "        # Draw rectangle and label\n",
    "        color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
    "        \n",
    "        # Display confidence score\n",
    "        confidence = 1 - best_distance\n",
    "        cv2.putText(frame, f\"{confidence:.2f}\", (left, bottom + 45), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def log_attendance(name):\n",
    "    \"\"\"Log recognized faces with timestamp\"\"\"\n",
    "    if name != UNKNOWN_NAME:\n",
    "        with open(\"attendance.csv\", \"a\") as f:\n",
    "            f.write(f\"{name},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    \n",
    "    if not known_encodings:\n",
    "        print(\"No face database found. Please create one first.\")\n",
    "        return\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame = recognize_faces(frame, known_encodings, known_names)\n",
    "        \n",
    "        # # Display FPS\n",
    "        # cv2.putText(frame, f\"FPS: {int(video_capture.get(cv2.CAP_PROP_FPS))}\", \n",
    "        #            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('a'):  # Add new face to database\n",
    "            add_new_face(frame, known_encodings, known_names)\n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 0.4  # Similarity threshold\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "MIN_FACE_SIZE = 100  # Minimum pixel size for a face to be considered\n",
    "BLUR_THRESHOLD = 100  # Threshold for blur detection (lower is more blurry)\n",
    "SCREEN_REFLECTION_THRESHOLD = 200  # Brightness threshold for screen detection\n",
    "FRAME_SKIP = 1   # Process every nth frame to reduce load\n",
    "RESIZE_FACTOR = 0.5  # Resize frame for faster processing (0.5 = half size)\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file with error handling\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            known_face_encodings, known_face_names = pickle.load(f)\n",
    "        return known_face_encodings, known_face_names\n",
    "    except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "        print(f\"Error loading face database: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def is_screen_face(frame, face_location):\n",
    "    \"\"\"Check if the face is likely on a screen (photo/video)\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    face_region = frame[top:bottom, left:right]\n",
    "    if face_region.size == 0:  # Skip if region is empty\n",
    "        return False\n",
    "    gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray_face)\n",
    "    return avg_brightness > SCREEN_REFLECTION_THRESHOLD\n",
    "\n",
    "def is_too_blurry(frame):\n",
    "    \"\"\"Check if the frame is too blurry for reliable detection\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() < BLUR_THRESHOLD\n",
    "\n",
    "def is_face_too_small(face_location):\n",
    "    \"\"\"Check if the detected face is too small to be real\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    return (right - left) < MIN_FACE_SIZE or (bottom - top) < MIN_FACE_SIZE\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names, frame_counter):\n",
    "    \"\"\"Optimized face recognition with filtering\"\"\"\n",
    "    # Skip processing for some frames to improve performance\n",
    "    if frame_counter % FRAME_SKIP != 0:\n",
    "        return frame, []\n",
    "    \n",
    "    # Resize frame for faster processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=RESIZE_FACTOR, fy=RESIZE_FACTOR)\n",
    "    \n",
    "    if is_too_blurry(small_frame):\n",
    "        cv2.putText(frame, \"Low quality - ignoring\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        return frame, []\n",
    "\n",
    "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    \n",
    "    recognized_names = []\n",
    "    \n",
    "    for face_location in face_locations:\n",
    "        # Scale back up face locations since the frame was resized\n",
    "        top, right, bottom, left = [int(coord / RESIZE_FACTOR) for coord in face_location]\n",
    "        \n",
    "        if is_face_too_small((top, right, bottom, left)) or is_screen_face(frame, (top, right, bottom, left)):\n",
    "            continue\n",
    "            \n",
    "        face_encoding = face_recognition.face_encodings(\n",
    "            rgb_small_frame, \n",
    "            [face_location]\n",
    "        )[0]\n",
    "        \n",
    "        # Only compare with known faces if we have any\n",
    "        if known_encodings:\n",
    "            face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "            best_match_idx = np.argmin(face_distances)\n",
    "            best_distance = face_distances[best_match_idx]\n",
    "            name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "        else:\n",
    "            name = UNKNOWN_NAME\n",
    "        \n",
    "        recognized_names.append(name)\n",
    "        \n",
    "        # Draw rectangle and label on original frame\n",
    "        color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
    "        \n",
    "        # Display confidence score if we have known faces\n",
    "        if known_encodings:\n",
    "            confidence = 1 - best_distance\n",
    "            cv2.putText(frame, f\"{confidence:.2f}\", (left, bottom + 45), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "    \n",
    "    return frame, recognized_names\n",
    "\n",
    "def log_attendance(name):\n",
    "    \"\"\"Log recognized faces with timestamp\"\"\"\n",
    "    if name != UNKNOWN_NAME:\n",
    "        with open(\"attendance.csv\", \"a\") as f:\n",
    "            f.write(f\"{name},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    \n",
    "    if not known_encodings:\n",
    "        print(\"No face database found. Please create one first.\")\n",
    "        return\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    # Try to enable hardware acceleration if available\n",
    "    try:\n",
    "        video_capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    frame_counter = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame_counter += 1\n",
    "        processed_frame, recognized_names = recognize_faces(frame, known_encodings, known_names, frame_counter)\n",
    "        \n",
    "        # Log attendance for recognized faces\n",
    "        for name in recognized_names:\n",
    "            log_attendance(name)\n",
    "        \n",
    "        # Display FPS\n",
    "        fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "        cv2.putText(processed_frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Face Recognition', processed_frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 0.4  # Similarity threshold\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "MIN_FACE_SIZE = 100  # Minimum pixel size for a face to be considered\n",
    "BLUR_THRESHOLD = 100  # Threshold for blur detection (lower is more blurry)\n",
    "SCREEN_REFLECTION_THRESHOLD = 200  # Brightness threshold for screen detection\n",
    "EDGE_VARIATION_THRESHOLD = 20  # Threshold for edge variation detection (screen faces tend to have less variation)\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file with error handling\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            known_face_encodings, known_face_names = pickle.load(f)\n",
    "        return known_face_encodings, known_face_names\n",
    "    except (FileNotFoundError, EOFError, pickle.UnpicklingError) as e:\n",
    "        print(f\"Error loading face database: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def is_screen_face(frame, face_location):\n",
    "    \"\"\"Enhanced check if the face is likely on a screen (photo/video)\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    face_region = frame[top:bottom, left:right]\n",
    "    \n",
    "    # Check brightness\n",
    "    gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "    avg_brightness = np.mean(gray_face)\n",
    "    \n",
    "    # Check edge variation (real faces have more complex edges)\n",
    "    edges = cv2.Canny(gray_face, 100, 200)\n",
    "    edge_variation = np.std(edges)\n",
    "    \n",
    "    # Check color saturation (screen faces often have lower saturation)\n",
    "    hsv = cv2.cvtColor(face_region, cv2.COLOR_BGR2HSV)\n",
    "    avg_saturation = np.mean(hsv[:,:,1])\n",
    "    \n",
    "    # Combine multiple factors\n",
    "    brightness_factor = avg_brightness > SCREEN_REFLECTION_THRESHOLD\n",
    "    edge_factor = edge_variation < EDGE_VARIATION_THRESHOLD\n",
    "    saturation_factor = avg_saturation < 50  # Lower saturation threshold\n",
    "    \n",
    "    # If at least two factors indicate a screen face\n",
    "    return (brightness_factor + edge_factor + saturation_factor) >= 2\n",
    "\n",
    "def is_too_blurry(frame):\n",
    "    \"\"\"Check if the frame is too blurry for reliable detection\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() < BLUR_THRESHOLD\n",
    "\n",
    "def is_face_too_small(face_location):\n",
    "    \"\"\"Check if the detected face is too small to be real\"\"\"\n",
    "    (top, right, bottom, left) = face_location\n",
    "    return (right - left) < MIN_FACE_SIZE or (bottom - top) < MIN_FACE_SIZE\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names):\n",
    "    \"\"\"Enhanced face recognition with filtering\"\"\"\n",
    "    if is_too_blurry(frame):\n",
    "        cv2.putText(frame, \"Low quality - ignoring\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        return frame\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    \n",
    "    for face_location in face_locations:\n",
    "        if is_face_too_small(face_location):\n",
    "            continue\n",
    "            \n",
    "        if is_screen_face(frame, face_location):\n",
    "            (top, right, bottom, left) = face_location\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, \"Screen Face Detected\", (left, bottom + 20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)\n",
    "            continue\n",
    "            \n",
    "        face_encoding = face_recognition.face_encodings(rgb_frame, [face_location])[0]\n",
    "        face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "        best_match_idx = np.argmin(face_distances)\n",
    "        best_distance = face_distances[best_match_idx]\n",
    "        \n",
    "        name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "        (top, right, bottom, left) = face_location\n",
    "        \n",
    "        # Draw rectangle and label\n",
    "        color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
    "        \n",
    "        # Display confidence score\n",
    "        confidence = 1 - best_distance\n",
    "        cv2.putText(frame, f\"{confidence:.2f}\", (left, bottom + 45), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "        \n",
    "        log_attendance(name)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "def log_attendance(name):\n",
    "    \"\"\"Log recognized faces with timestamp\"\"\"\n",
    "    if name != UNKNOWN_NAME:\n",
    "        with open(\"attendance.csv\", \"a\") as f:\n",
    "            f.write(f\"{name},{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    \n",
    "    if not known_encodings:\n",
    "        print(\"No face database found. Please create one first.\")\n",
    "        return\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame = recognize_faces(frame, known_encodings, known_names)\n",
    "        \n",
    "        cv2.imshow('Face Recognition', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 133 known faces\n",
      "Starting face recognition with CNN model (Threshold: 0.6)\n",
      "Error: Could not read frame\n",
      "Face recognition stopped\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 0.6  # Increased threshold for better accuracy with CNN\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "CHALLENGE_DURATION = 10\n",
    "PENALTY_DURATION = 15\n",
    "CHALLENGE_COOLDOWN = 10\n",
    "USE_CNN_MODEL = True  # Using CNN model\n",
    "DETECTION_SCALE = 0.25  # Scale for face detection (keeps encoding at full resolution)\n",
    "PROCESS_EVERY_N_FRAMES = 1  # Process every frame for best accuracy\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            known_face_encodings, known_face_names = pickle.load(f)\n",
    "        print(f\"Loaded {len(known_face_encodings)} known faces\")\n",
    "        return known_face_encodings, known_face_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading known faces: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def detect_faces_cnn(frame):\n",
    "    \"\"\"Detect faces using CNN model with proper scaling\"\"\"\n",
    "    # Convert to RGB (face_recognition uses RGB)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize frame for faster detection while maintaining aspect ratio\n",
    "    small_frame = cv2.resize(rgb_frame, (0, 0), fx=DETECTION_SCALE, fy=DETECTION_SCALE)\n",
    "    \n",
    "    # Find face locations with CNN model\n",
    "    face_locations = face_recognition.face_locations(\n",
    "        small_frame,\n",
    "        model=\"cnn\",\n",
    "        number_of_times_to_upsample=1  # Reduce upsampling for speed\n",
    "    )\n",
    "    \n",
    "    # Scale face locations back to original frame size\n",
    "    return [\n",
    "        (int(top/DETECTION_SCALE), int(right/DETECTION_SCALE), \n",
    "        int(bottom/DETECTION_SCALE), int(left/DETECTION_SCALE))\n",
    "        for (top, right, bottom, left) in face_locations\n",
    "    ], rgb_frame\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names, challenge_status, penalty_list):\n",
    "    \"\"\"Recognize faces with proper CNN implementation\"\"\"\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Detect faces using CNN\n",
    "    face_locations, rgb_frame = detect_faces_cnn(frame)\n",
    "    \n",
    "    # Get face encodings from the original resolution\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    \n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # Check if face is in penalty\n",
    "        in_penalty = False\n",
    "        for penalty in penalty_list:\n",
    "            if current_time - penalty['time'] < PENALTY_DURATION:\n",
    "                distance = face_recognition.face_distance([penalty['encoding']], face_encoding)[0]\n",
    "                if distance < 0.5:\n",
    "                    in_penalty = True\n",
    "                    break\n",
    "        \n",
    "        if in_penalty:\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"Unknown (Penalty)\", (left, bottom + 20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "            continue\n",
    "        \n",
    "        # Find matches from known faces\n",
    "        matches = face_recognition.compare_faces(known_encodings, face_encoding, tolerance=THRESHOLD)\n",
    "        face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "        \n",
    "        best_match_idx = np.argmin(face_distances)\n",
    "        best_distance = face_distances[best_match_idx]\n",
    "        \n",
    "        if matches[best_match_idx] and best_distance <= THRESHOLD:\n",
    "            name = known_names[best_match_idx]\n",
    "            color = (0, 255, 0)  # Green\n",
    "            \n",
    "            # Challenge system logic\n",
    "            needs_challenge = (\n",
    "                name not in challenge_status or \n",
    "                (not challenge_status[name]['active'] and \n",
    "                 current_time - challenge_status[name]['last_challenge'] > CHALLENGE_COOLDOWN)\n",
    "            )\n",
    "            \n",
    "            if needs_challenge:\n",
    "                challenge_status[name] = {\n",
    "                    'active': True,\n",
    "                    'text': get_challenge(),\n",
    "                    'start_time': current_time,\n",
    "                    'last_challenge': current_time,\n",
    "                    'prev_frame': frame.copy(),\n",
    "                    'encoding': face_encoding\n",
    "                }\n",
    "        else:\n",
    "            name = UNKNOWN_NAME\n",
    "            color = (0, 0, 255)  # Red\n",
    "        \n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    return frame, challenge_status\n",
    "\n",
    "def main():\n",
    "    # Load known faces\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    if not known_encodings:\n",
    "        print(\"No known faces loaded. Please check your face_encodings.pkl file\")\n",
    "        return\n",
    "    \n",
    "    # Initialize video capture\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video source\")\n",
    "        return\n",
    "    \n",
    "    challenge_status = {}\n",
    "    penalty_list = []\n",
    "    frame_counter = 0\n",
    "    \n",
    "    print(f\"Starting face recognition with CNN model (Threshold: {THRESHOLD})\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame\")\n",
    "                break\n",
    "            \n",
    "            frame_counter += 1\n",
    "            if frame_counter % PROCESS_EVERY_N_FRAMES != 0:\n",
    "                continue\n",
    "            \n",
    "            # Process frame\n",
    "            processed_frame, challenge_status = recognize_faces(\n",
    "                frame, known_encodings, known_names, challenge_status, penalty_list\n",
    "            )\n",
    "            \n",
    "            # Display challenge status if active\n",
    "            current_time = time.time()\n",
    "            for name, status in list(challenge_status.items()):\n",
    "                if status['active']:\n",
    "                    cv2.putText(processed_frame, f\"Challenge: {status['text']}\", (20, 50), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "                    remaining = CHALLENGE_DURATION - (current_time - status['start_time'])\n",
    "                    cv2.putText(processed_frame, f\"Time left: {max(0, round(remaining, 1))}s\", \n",
    "                               (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "                    \n",
    "                    if check_challenge(status['text'], status['prev_frame'], processed_frame):\n",
    "                        challenge_status[name]['active'] = False\n",
    "                        print(f\"{name} passed challenge!\")\n",
    "                    elif current_time - status['start_time'] > CHALLENGE_DURATION:\n",
    "                        challenge_status[name]['active'] = False\n",
    "                        penalty_list.append({'encoding': status['encoding'], 'time': current_time})\n",
    "                        print(f\"{name} failed challenge! Penalty active\")\n",
    "            \n",
    "            # Cleanup expired penalties\n",
    "            penalty_list = [p for p in penalty_list if current_time - p['time'] < PENALTY_DURATION]\n",
    "            \n",
    "            # Display\n",
    "            cv2.imshow('Face Recognition (CNN)', processed_frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Face recognition stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "\n",
    "# Configuration\n",
    "THRESHOLD = 0.45\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "EYE_AR_THRESH = 0.25  # Eye Aspect Ratio threshold (adjust as needed)\n",
    "EYE_AR_CONSEC_FRAMES = 2  # Number of consecutive frames to confirm blink\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        known_face_encodings, known_face_names = pickle.load(f)\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    \"\"\"Compute Eye Aspect Ratio (EAR) to detect blinking\"\"\"\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names, counter=0, total=0):\n",
    "    \"\"\"Recognize faces with liveness check\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    \n",
    "    # Initialize face landmarks detector (for eye blink detection)\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame, face_locations)\n",
    "    \n",
    "    for (top, right, bottom, left), face_encoding, landmarks in zip(\n",
    "        face_locations, face_encodings, face_landmarks_list\n",
    "    ):\n",
    "        # Check for blinking (liveness detection)\n",
    "        left_eye = landmarks[\"left_eye\"]\n",
    "        right_eye = landmarks[\"right_eye\"]\n",
    "        left_ear = eye_aspect_ratio(left_eye)\n",
    "        right_ear = eye_aspect_ratio(right_eye)\n",
    "        ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            counter += 1\n",
    "        else:\n",
    "            if counter >= EYE_AR_CONSEC_FRAMES:\n",
    "                total += 1\n",
    "            counter = 0\n",
    "\n",
    "        # Only recognize if blink detected (anti-spoofing)\n",
    "        if total > 0:\n",
    "            face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "            best_match_idx = np.argmin(face_distances)\n",
    "            best_distance = face_distances[best_match_idx]\n",
    "            name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "            color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "        else:\n",
    "            name = \"Blink to verify\"\n",
    "            color = (255, 255, 0)  # Yellow (pending verification)\n",
    "\n",
    "        # Draw face box and label\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    return frame, counter, total\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    counter = 0\n",
    "    total = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame, counter, total = recognize_faces(\n",
    "            frame, known_encodings, known_names, counter, total\n",
    "        )\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from scipy.spatial import distance as dist\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Configurations\n",
    "THRESHOLD = 0.45\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "EYE_AR_THRESH = 0.25  # Eye Aspect Ratio threshold\n",
    "EYE_AR_CONSEC_FRAMES = 3  # Frames to confirm blink\n",
    "REQUIRED_BLINKS = 2  # Minimum blinks to verify liveness\n",
    "HEAD_MOVEMENT_THRESH = 15  # Pixel movement for head motion\n",
    "TEXTURE_THRESH = 10  # LBP mean threshold (lower = smoother, likely fake)\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    \"\"\"Load known face encodings and names from file\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        known_face_encodings, known_face_names = pickle.load(f)\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    \"\"\"Compute Eye Aspect Ratio (EAR) for blink detection\"\"\"\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def analyze_skin_texture(face_roi):\n",
    "    \"\"\"Check if skin texture is real using Local Binary Patterns (LBP)\"\"\"\n",
    "    gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, 8, 1, method=\"uniform\")\n",
    "    texture_score = np.mean(lbp)\n",
    "    return texture_score > TEXTURE_THRESH  # True if real skin\n",
    "\n",
    "def challenge_user(frame, face_location):\n",
    "    \"\"\"Ask user to perform actions (blink/head movement)\"\"\"\n",
    "    top, right, bottom, left = face_location\n",
    "    cv2.putText(frame, \"Blink or move your head!\", (left, top - 10),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "    return frame\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names, liveness_passed=False):\n",
    "    \"\"\"Recognize faces with liveness + texture checks\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    face_landmarks = face_recognition.face_landmarks(rgb_frame, face_locations)\n",
    "\n",
    "    for (top, right, bottom, left), face_encoding, landmarks in zip(\n",
    "        face_locations, face_encodings, face_landmarks\n",
    "    ):\n",
    "        # Step 1: Liveness Check (Blink/Head Movement)\n",
    "        if not liveness_passed:\n",
    "            frame = challenge_user(frame, (top, right, bottom, left))\n",
    "            left_ear = eye_aspect_ratio(landmarks[\"left_eye\"])\n",
    "            right_ear = eye_aspect_ratio(landmarks[\"right_eye\"])\n",
    "            ear = (left_ear + right_ear) / 2.0\n",
    "            is_blinking = ear < EYE_AR_THRESH\n",
    "\n",
    "            # Simulate head movement check (replace with actual tracking)\n",
    "            has_moved = np.random.random() > 0.5  # Mock logic\n",
    "\n",
    "            if is_blinking or has_moved:\n",
    "                liveness_passed = True\n",
    "                cv2.putText(frame, \"Liveness passed!\", (left, bottom + 40),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # Step 2: Face Recognition (After Liveness)\n",
    "        if liveness_passed:\n",
    "            face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "            best_match_idx = np.argmin(face_distances)\n",
    "            best_distance = face_distances[best_match_idx]\n",
    "            name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "            color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "\n",
    "            # Step 3: Skin Texture Check (After Recognition)\n",
    "            face_roi = frame[top:bottom, left:right]\n",
    "            is_real_skin = analyze_skin_texture(face_roi)\n",
    "            if not is_real_skin:\n",
    "                name = \"FAKE (Texture)\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            cv2.putText(frame, name, (left, bottom + 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "\n",
    "    return frame, liveness_passed\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    liveness_passed = False\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame, liveness_passed = recognize_faces(frame, known_encodings, known_names, liveness_passed)\n",
    "        cv2.imshow(\"Anti-Spoof Face Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texture_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, name, (left, bottom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20\u001b[39m),\n\u001b[0;32m     53\u001b[0m                    cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, color, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTexture Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtexture_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add inside analyze_skin_texture()\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     59\u001b[0m     known_encodings, known_names \u001b[38;5;241m=\u001b[39m load_known_faces(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_encodings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texture_score' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Configurations\n",
    "THRESHOLD = 0.45\n",
    "UNKNOWN_NAME = \"Unknown\"\n",
    "TEXTURE_THRESH = 20  # Lowered threshold (adjust based on testing)\n",
    "BLUR_THRESH = 50   # Discard blurred faces (variance threshold)\n",
    "\n",
    "def load_known_faces(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def analyze_skin_texture(face_roi):\n",
    "    \"\"\"Improved texture analysis with blur check.\"\"\"\n",
    "    gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 1. Check if face is too blurry (avoid false positives)\n",
    "    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    if blur_score < BLUR_THRESH:\n",
    "        return False  # Skip if face is blurry\n",
    "\n",
    "    # 2. Analyze texture (LBP)\n",
    "    lbp = local_binary_pattern(gray, 8, 1, method=\"uniform\")\n",
    "    texture_score = np.mean(lbp)\n",
    "    return texture_score > TEXTURE_THRESH  # True if real skin\n",
    "\n",
    "def recognize_faces(frame, known_encodings, known_names):\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # Step 1: Face recognition\n",
    "        face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "        best_match_idx = np.argmin(face_distances)\n",
    "        best_distance = face_distances[best_match_idx]\n",
    "        name = known_names[best_match_idx] if best_distance <= THRESHOLD else UNKNOWN_NAME\n",
    "        color = (0, 255, 0) if name != UNKNOWN_NAME else (0, 0, 255)\n",
    "\n",
    "        # Step 2: Skin texture check (only for recognized faces)\n",
    "        if name != UNKNOWN_NAME:\n",
    "            face_roi = frame[top:bottom, left:right]\n",
    "            is_real_skin = analyze_skin_texture(face_roi)\n",
    "            if not is_real_skin:\n",
    "                name = \"FAKE (Texture)\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n",
    "        cv2.putText(frame, name, (left, bottom + 20),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def main():\n",
    "    known_encodings, known_names = load_known_faces(\"face_encodings.pkl\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = recognize_faces(frame, known_encodings, known_names)\n",
    "        cv2.imshow(\"Face Recognition + Anti-Spoof\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
